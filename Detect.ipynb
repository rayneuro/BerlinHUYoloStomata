{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the pytorch and ultralytics \n",
    "%pip install -U ultralytics\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.1 ðŸš€ Python-3.11.4 torch-2.2.0.dev20230921 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "Setup complete âœ… (24 CPUs, 125.5 GB RAM, 1121.3/1876.2 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import pandas as pd\n",
    "from distutils.dir_util import copy_tree\n",
    "import os\n",
    "# Check  GPU  and pytorch works\n",
    "ultralytics.checks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: \n",
    "### Puts the images folders in DetectData and enter the folder name.\n",
    "\n",
    "\n",
    "```bash\n",
    "/DetectData/\n",
    "â””â”€â”€ your_images_folder/  \n",
    "    â”œâ”€â”€ [ex : 001.jpg,001.png] \n",
    "    ......\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the code loading into the pretrain model weight with T16 + T32 + Giessen datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 code area  \n",
    "model = YOLO(\"./T22without_pipeline.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please follow the step in detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No images or videos found in ./DetectData/test. Supported formats are:\nimages: {'dng', 'jpg', 'jpeg', 'tif', 'tiff', 'pfm', 'mpo', 'png', 'webp', 'bmp'}\nvideos: {'avi', 'asf', 'mpg', 'mpeg', 'm4v', 'mkv', 'mp4', 'gif', 'webm', 'mov', 'ts', 'wmv'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m name_of_folder\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# project\tstr\t    None\tName of the project directory where prediction outputs are saved if save is enabled.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# name\t    str\t    None\tName of the prediction run. Used for creating a subdirectory within the project folder, where prediction outputs are stored if save is enabled.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save in the folder runs/obb/predict/ [name of the folder your select]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(folder_path,save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, save_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m ,save_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, show_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m , name \u001b[38;5;241m=\u001b[39m name_of_folder)\n",
      "File \u001b[0;32m~/BerlinHUYoloStomata/ultralytics/engine/model.py:555\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/BerlinHUYoloStomata/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/BerlinHUYoloStomata/ultralytics/engine/predictor.py:226\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_source(source \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msource)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[0;32m~/BerlinHUYoloStomata/ultralytics/engine/predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m )\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m load_inference_source(\n\u001b[1;32m    199\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[1;32m    200\u001b[0m     batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    201\u001b[0m     vid_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvid_stride,\n\u001b[1;32m    202\u001b[0m     buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mstream_buffer,\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[1;32m    210\u001b[0m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[0;32m~/BerlinHUYoloStomata/ultralytics/data/build.py:202\u001b[0m, in \u001b[0;36mload_inference_source\u001b[0;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadPilAndNumpy(source)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadImagesAndVideos(source, batch\u001b[38;5;241m=\u001b[39mbatch, vid_stride\u001b[38;5;241m=\u001b[39mvid_stride)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Attach source types to the dataset\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28msetattr\u001b[39m(dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_type)\n",
      "File \u001b[0;32m~/BerlinHUYoloStomata/ultralytics/data/loaders.py:316\u001b[0m, in \u001b[0;36mLoadImagesAndVideos.__init__\u001b[0;34m(self, path, batch, vid_stride)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images or videos found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFORMATS_HELP_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No images or videos found in ./DetectData/test. Supported formats are:\nimages: {'dng', 'jpg', 'jpeg', 'tif', 'tiff', 'pfm', 'mpo', 'png', 'webp', 'bmp'}\nvideos: {'avi', 'asf', 'mpg', 'mpeg', 'm4v', 'mkv', 'mp4', 'gif', 'webm', 'mov', 'ts', 'wmv'}"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1 code area\n",
    "# Use GUI Select Folder\n",
    "# Put the images you want to predict in the folder\n",
    "\n",
    "name_of_folder = input('Please enter folder name: ')\n",
    "path = './DetectData/'\n",
    "\n",
    "\n",
    "folder_path = path + name_of_folder\n",
    "\n",
    "# project\tstr\t    None\tName of the project directory where prediction outputs are saved if save is enabled.\n",
    "# name\t    str\t    None\tName of the prediction run. Used for creating a subdirectory within the project folder, where prediction outputs are stored if save is enabled.\n",
    "\n",
    "# Save in the folder runs/obb/predict/[name of the folder your select]\n",
    "\n",
    "result = model.predict(folder_path,save = True, save_txt = True ,save_conf = True, show_conf = False , name = name_of_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "### Converting YOLOv8 Predict OBB Format `.txt` Files to `.csv`\n",
    "\n",
    "This guide explains how to convert YOLOv8 Oriented Bounding Box (OBB) format `.txt` files into a `.csv` file format for easier analysis or processing.\n",
    "\n",
    "### YOLOv8 OBB `.txt` File Format\n",
    "\n",
    "In the YOLOv8 OBB `.txt` format, each line in the file corresponds to a detected object in an image, with the following structure:\n",
    "\n",
    "Where:\n",
    "- `class_id`: The class label of the detected object.\n",
    "- `x1, y1`, `x2, y2`, `x3, y3`, `x4, y4`: The coordinates of the four corners of the oriented bounding box.\n",
    "- `confidence`: The confidence score of the detection.\n",
    "\n",
    "\n",
    "Convert to\n",
    "- `center x` , `center y` : The center of the rotated bounding box\n",
    "- `w` , `h` : the width and height of rotated bounding box  \n",
    "- `angle` : the angle of rotate\n",
    "\n",
    "The final output file is Result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2 code area\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert the coordinate to cx ,cy , h , w\n",
    "def get_angle(x1,y1,x2,y2,x3,y3,x4,y4):\n",
    "    if x2-x1 != 0 and x4-x3 != 0:\n",
    "        theta = (math.atan((y2-y1)/(x2-x1)) + math.atan((y4-y3)/(x4-x3)))/2\n",
    "    else:\n",
    "        theta = np.pi/2\n",
    "\n",
    "    if theta >= 0:\n",
    "        angle = theta\n",
    "    else:\n",
    "        angle = theta + np.pi\n",
    "    return angle,theta\n",
    "\n",
    "\n",
    "def get_w_and_h(x1,y1,x2,y2,x3,y3,x4,y4):\n",
    "    w = math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "    h = math.sqrt((x1-x3)**2+(y1-y3)**2)\n",
    "    return w,h\n",
    "\n",
    "def get_x_and_y(x1,y1,x2,y2,x3,y3,x4,y4,theta,w,h):\n",
    "    cx = (x2+x3)/2\n",
    "    cy = (y2+y3)/2\n",
    "    x = cx - w/2\n",
    "    y = cy - h/2\n",
    "    return x,y,cx,cy\n",
    "\n",
    "\n",
    "def get_newest_folder(path):\n",
    "  # Get a list of all directories in the specified path\n",
    "  dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "  # Sort the directories based on creation time\n",
    "  dirs.sort(key=lambda d: os.path.getctime(os.path.join(path, d)), reverse=True)\n",
    "  # Return the newest folder\n",
    "  if dirs:\n",
    "    return dirs[0]\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "path_to_search = \"runs/obb/\"\n",
    "newest_folder = get_newest_folder(path_to_search)\n",
    "\n",
    "labels = [os.path.join(path_to_search+newest_folder+'/labels', x) for x in os.listdir(path_to_search+newest_folder+'/labels') if x[-3:] == \"txt\"]\n",
    "#labelsname = [f for f in os.listdir('./runs/obb/predict/labels')]\n",
    "\n",
    "labels.sort()\n",
    "#labelsname.sort()\n",
    "\n",
    "# Convert and save the annotations\n",
    "\n",
    "class_name_to_id_mapping = {\"complete\": 0,\n",
    "                \"incomplete\": 1,\n",
    "                           \"blurry.complete\": 2,\n",
    "                           \"blurry.incomplete\" :3 ,\n",
    "                           \"hair\":4\n",
    "                           } # human hair\n",
    "\n",
    "id_to_class_name_mapping = dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys()))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[ 'class' ,'boundingbox_x' , 'boundingbox_y','boundingbox_width' ,'boundingbox_height' , 'angle' , 'confidence', 'File Name' ])\n",
    "\n",
    "# Read txt label file\n",
    "for name in labels:\n",
    "    fr_line = open(name)\n",
    "    for bbox_line in fr_line.readlines():\n",
    "            info = bbox_line.strip().split(' ')\n",
    "            cls= int(info[0])\n",
    "            x1,y1,x2,y2,x3,y3,x4,y4 = float(info[1]) , float(info[2]) , float(info[3]) , float(info[4]) , float(info[5]) , float(info[6]) , float(info[7]) , float(info[8])\n",
    "            conf = float(info[9])\n",
    "            file_name = name.split('/')[-1]\n",
    "            angle,theta = get_angle(x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "            w,h = get_w_and_h(x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "            x,y,cx,cy = get_x_and_y(x1,y1,x2,y2,x3,y3,x4,y4,theta,w,h)\n",
    "            new_row = {'class': id_to_class_name_mapping[cls] , 'boundingbox_x' : cx, 'boundingbox_y' : cy , 'boundingbox_width' : w, 'boundingbox_height' : h, 'angle': angle, 'confidence' : conf , 'File Name' : file_name }\n",
    "            df.loc[len(df)] = new_row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the Dataframe to csv file\n",
    "#save_name = 'folder_of_name.csv'\n",
    "df.to_csv( './result/'+path.split('/')[-1]+'.csv', encoding = 'utf-8' ,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
